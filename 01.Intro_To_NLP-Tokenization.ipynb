{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a68363-134f-4168-87aa-4a72f7fdb3fc",
   "metadata": {},
   "source": [
    "# üìò What is Natural Language Processing (NLP)?\n",
    "\n",
    "What we are doing right now is **natural language processing** ‚Äî you're listening to the words and sentences I'm forming, and you're forming some kind of understanding from them.\n",
    "\n",
    "When we ask a **computer** to do the same, it‚Äôs called **Natural Language Processing (NLP).**\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Example:\n",
    "\n",
    "**Input (Unstructured):**  \n",
    "`Add eggs and milk to my shopping list.`\n",
    "\n",
    "This is *unstructured data* for machines.\n",
    "\n",
    "Computers understand information in **structured formats**, such as lists or other data structures.\n",
    "\n",
    "**Equivalent Structured Data (XML format):**\n",
    "```xml\n",
    "<shopping_list>\n",
    "    <item>Eggs</item>\n",
    "    <item>Milk</item>\n",
    "</shopping_list>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453dc639-09f7-445c-9629-4241dc172756",
   "metadata": {},
   "source": [
    "# üöÄ Applications of NLP\n",
    "#### 1. üîÑ Machine Translation\n",
    "Translating text or speech from one language to another.\n",
    "\n",
    "#### 2. ü§ñ Chatbots or Virtual Assistants\n",
    "Understanding and responding to user queries in natural language.\n",
    "\n",
    "#### 3. üí¨ Sentiment Analysis\n",
    "Analyzing customer reviews, emails, or feedback to determine emotional tone.\n",
    "\n",
    "#### 4. üö´ Spam Detection\n",
    "Identifying unwanted or harmful messages by analyzing text for: False promises, Unnecessary urgency, Malicious links, Requests for personal information\n",
    "\n",
    "# ‚öôÔ∏è Steps in NLP\n",
    "Natural Language Processing typically involves the following key steps:\n",
    "\n",
    "### 1Ô∏è‚É£ Tokenization\n",
    "Breaking a sentence into smaller units called tokens (usually words or subwords).\n",
    "\n",
    "Example: \"Add eggs and milk\" ‚Üí [\"Add\", \"eggs\", \"and\", \"milk\"]\n",
    "\n",
    "### 2Ô∏è‚É£ Stemming\n",
    "Reducing words to their root form by trimming suffixes/prefixes.\n",
    "\n",
    "Example: \"running\", \"ran\", \"runs\" ‚Üí run\n",
    "\n",
    "‚ùó Note: Stemming can be crude or inaccurate:\n",
    "\n",
    "\"university\" and \"universal\" do not reduce to \"universe\"\n",
    "\n",
    "### 3Ô∏è‚É£ Lemmatization\n",
    "Identifies the dictionary root (lemma) of a word based on its context and meaning.\n",
    "\n",
    "Example: \"better\" ‚Üí good (correct lemma)\n",
    "Stemming version: \"better\" ‚Üí \"bet\" (inaccurate)\n",
    "\n",
    "‚úÖ Lemmatization is more accurate and meaningful than stemming.\n",
    "\n",
    "### 4Ô∏è‚É£ Part of Speech (POS) Tagging\n",
    "Assigns a grammatical role to each token in context.\n",
    "\n",
    "Example:\n",
    "\"book\" as a noun ‚Üí I read a book.\n",
    "\"book\" as a verb ‚Üí Please book a table.\n",
    "\n",
    "### 5Ô∏è‚É£ Named Entity Recognition (NER)\n",
    "Detects and classifies named entities (proper nouns) in text into categories like:\n",
    "\n",
    "üë© Person ‚Äî e.g., Maria\n",
    "\n",
    "üåç Location ‚Äî e.g., London\n",
    "\n",
    "üè¢ Organization ‚Äî e.g., Google\n",
    "\n",
    "üìÖ Date ‚Äî e.g., July 5, 2025\n",
    "\n",
    "NER is useful in extracting structured data from unstructured text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73242fc7-aa38-470f-9184-336244fb13e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.7\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add66cee-f747-4972-bf89-91ea0512198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk, version 3.9.1\n"
     ]
    }
   ],
   "source": [
    "! nltk --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6be10af-b2e3-4dc8-a5fb-a7fdfabd43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca276b-d507-48c1-b9f0-131db3a868e7",
   "metadata": {},
   "source": [
    "## Tokenization - \n",
    "#### Breaking a sentence into smaller units called tokens (usually words or subwords)\n",
    "\n",
    "word_tokenize = Break sentence or paragraph into small tokens(words, punctuation, symbols, numbers, etc)\n",
    "sent_tokenize = Break paragraphs or string into Sentences(Usually split where delimiter is full stop(.) in the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15bfcbf3-41d5-4146-9e42-fe4532aa9d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'a', '$', '50,000', 'mortgage', 'of', '30', 'years', 'at', '8', 'percent', ',', 'the', 'monthly', 'payment', 'would', 'be', '$', '366.88', '.']\n"
     ]
    }
   ],
   "source": [
    "s1 = \"On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.\"\n",
    "result = nltk.word_tokenize(s1)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b4bfa43-dfa7-44b3-9dfd-2d159be32eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'We', 'beat', 'some', 'pretty', 'good', 'teams', 'to', 'get', 'here', ',', \"''\", 'Slocum', 'said', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use of Escape Characters \\ - only double quotes are consider \"\n",
    "s2 = \"\\\"We beat some pretty good teams to get here,\\\" Slocum said.\"\n",
    "result = nltk.word_tokenize(s2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1b74190-6aa0-438d-b306-301dcf3635f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Well', ',', 'we', 'could', \"n't\", 'have', 'this', 'predictable', ',', 'cliche-ridden', ',', '``', 'Touched', 'by', 'an', 'Angel', \"''\", '(', 'a', 'show', 'creator', 'John', 'Masius', 'worked', 'on', ')', 'wanna-be', 'if', 'she', 'did', \"n't\", '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Note - How couldn't is interpreted as two different words and created 2 tokens. e.g. 'could', \"n't\"\n",
    "# Note - Hypen join words are considered as single token e.g cliche-ridden, wanna-be\n",
    "s3 = \"Well, we couldn't have this predictable, cliche-ridden, \\\"Touched by an Angel\\\" (a show creator John Masius worked on) wanna-be if she didn't.\"\n",
    "result = nltk.word_tokenize(s3)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2349cec9-09d2-4837-b945-6406738de7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'can', 'not', 'can', 'not', 'work', 'under', 'these', 'conditions', '!']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Note - repetative words are considered meaningful, also cannot = can + not ie. 2 tokens are created\n",
    "s4 = \"I cannot cannot work under these conditions!\"\n",
    "result = nltk.word_tokenize(s4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "667f9c4c-3ff8-48f4-bddc-2b5d8865bf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'company', 'spent', '40.75', '%', 'of', 'its', 'income', 'last', 'year', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Note observation about number, percentage and year.\n",
    "s5 = \"The company spent 40.75% of its income last year.\"\n",
    "result = nltk.word_tokenize(s5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6de971fe-d3de-45bf-8ea2-e32e85537ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'arrived', 'at', '3:00', 'pm', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Note observation about time.\n",
    "s6 = \"He arrived at 3:00 pm.\"\n",
    "result = nltk.word_tokenize(s6)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd247bc-d2ea-42a4-a48e-d7330c6a0b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'bought', 'these', 'items', ':', 'books', ',', 'pencils', ',', 'and', 'pens', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Note Observation about : colon\n",
    "s7 = \"I bought these items: books, pencils, and pens.\"\n",
    "result = nltk.word_tokenize(s7)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90f9f27d-ea38-4877-97b0-c3df62e9df97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Though', 'there', 'were', '150', ',', '100', 'of', 'them', 'were', 'old', '.']\n",
      "['There', 'were', '300,000', ',', 'but', 'that', 'was', \"n't\", 'enough', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Note observation about number separated by comma and space are considered different number\n",
    "s8 = \"Though there were 150, 100 of them were old.\"\n",
    "result = nltk.word_tokenize(s8)\n",
    "print(result)\n",
    "\n",
    "# Note observation about comma without space in number is considered as number formatting.\n",
    "s9 = \"There were 300,000, but that wasn't enough.\"\n",
    "result = nltk.word_tokenize(s9)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "149ea3cf-4e8c-4929-afcb-348f41a2aae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'more', \"'n\", 'enough', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Note observatio about more'n\n",
    "s10 = \"It's more'n enough.\"\n",
    "result = nltk.word_tokenize(s10)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2728d85-33d2-4e43-9d59-37def74c4e73",
   "metadata": {},
   "source": [
    "# üß† Gathering Spans of Tokenized Strings in NLTK (Python)\n",
    "\n",
    "## ‚ú® What are Token Spans?\n",
    "\n",
    "When you tokenize a sentence using `nltk.tokenize.word_tokenize()`, you get the **individual tokens** (words, punctuation marks, etc.).\n",
    "\n",
    "But what if you want to know the **start and end position (span)** of each token in the original string?\n",
    "\n",
    "üëâ This is useful for tasks like:\n",
    "- Highlighting tokens in the original text\n",
    "- Mapping tokens to character offsets\n",
    "- Building annotated datasets\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Tool: `nltk.tokenize.TreebankWordTokenizer().span_tokenize()`\n",
    "\n",
    "The `span_tokenize()` method from `TreebankWordTokenizer` provides **(start, end)** character index spans for each token **relative to the original sentence**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Example Code:\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cb6f956-07ed-42fa-abe7-c51e0f617549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'On'\t\tSpan: (0, 2)\t\t\tOriginal Text: 'On'\n",
      "Token: 'a'\t\tSpan: (3, 4)\t\t\tOriginal Text: 'a'\n",
      "Token: '$'\t\tSpan: (5, 6)\t\t\tOriginal Text: '$'\n",
      "Token: '50,000'\t\tSpan: (6, 12)\t\t\tOriginal Text: '50,000'\n",
      "Token: 'mortgage'\t\tSpan: (13, 21)\t\t\tOriginal Text: 'mortgage'\n",
      "Token: 'of'\t\tSpan: (22, 24)\t\t\tOriginal Text: 'of'\n",
      "Token: '30'\t\tSpan: (25, 27)\t\t\tOriginal Text: '30'\n",
      "Token: 'years'\t\tSpan: (28, 33)\t\t\tOriginal Text: 'years'\n",
      "Token: 'at'\t\tSpan: (34, 36)\t\t\tOriginal Text: 'at'\n",
      "Token: '8'\t\tSpan: (37, 38)\t\t\tOriginal Text: '8'\n",
      "Token: 'percent'\t\tSpan: (39, 46)\t\t\tOriginal Text: 'percent'\n",
      "Token: ','\t\tSpan: (46, 47)\t\t\tOriginal Text: ','\n",
      "Token: 'the'\t\tSpan: (48, 51)\t\t\tOriginal Text: 'the'\n",
      "Token: 'monthly'\t\tSpan: (52, 59)\t\t\tOriginal Text: 'monthly'\n",
      "Token: 'payment'\t\tSpan: (60, 67)\t\t\tOriginal Text: 'payment'\n",
      "Token: 'would'\t\tSpan: (68, 73)\t\t\tOriginal Text: 'would'\n",
      "Token: 'be'\t\tSpan: (74, 76)\t\t\tOriginal Text: 'be'\n",
      "Token: '$'\t\tSpan: (77, 78)\t\t\tOriginal Text: '$'\n",
      "Token: '366.88'\t\tSpan: (78, 84)\t\t\tOriginal Text: '366.88'\n",
      "Token: '.'\t\tSpan: (84, 85)\t\t\tOriginal Text: '.'\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# Input text\n",
    "text = \"On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Get tokens and spans\n",
    "tokens = tokenizer.tokenize(text)\n",
    "spans = list(tokenizer.span_tokenize(text))\n",
    "\n",
    "# Display\n",
    "for token, span in zip(tokens, spans):\n",
    "    print(f\"Token: '{token}'\\t\\tSpan: {span}\\t\\t\\tOriginal Text: '{text[span[0]:span[1]]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfac137-d252-416c-818b-4350280c21c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Understanding the Output\n",
    "- Each span is a tuple (start_index, end_index):\n",
    "- These indexes refer to positions in the original string.\n",
    "- You can use them like text[start:end] to extract the token from the raw input.\n",
    "\n",
    "For example:\n",
    "\n",
    "    text[0:2] ‚Üí 'On'\n",
    "    text[5:6] ‚Üí '$'\n",
    "    text[84:91] ‚Üí '366.88'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2045e0fb-e5e1-4d9b-a1fd-f8de544ac6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '''Good muffins cost $3.88\\nin New (York).  Please (buy) me\\ntwo of them.\\n(Thanks).'''\n",
    "\n",
    "expected = [(0, 4), (5, 12), (13, 17), (18, 19), (19, 23),\n",
    "            (24, 26), (27, 30), (31, 32), (32, 36), (36, 37), (37, 38),\n",
    "            (40, 46), (47, 48), (48, 51), (51, 52), (53, 55), (56, 59),\n",
    "            (60, 62), (63, 68), (69, 70), (70, 76), (76, 77), (77, 78)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "170ad596-1d75-4f2f-a129-30eeb91798da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.NLTKWordTokenizer().span_tokenize(s)) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7763569c-0432-4e10-ab6d-bace9568012e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', '(', 'York', ')', '.', 'Please', '(', 'buy', ')', 'me', 'two', 'of', 'them.', '(', 'Thanks', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "expected = ['Good', 'muffins', 'cost', '$', '3.88', 'in',\n",
    "'New', '(', 'York', ')', '.', 'Please', '(', 'buy', ')',\n",
    "'me', 'two', 'of', 'them.', '(', 'Thanks', ')', '.']\n",
    "\n",
    "# from string s get the span list of tokens and compare with the above expected list\n",
    "l1 = [s[start:end] for start, end in nltk.NLTKWordTokenizer().span_tokenize(s)]\n",
    "print(l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8a4d7e4-123b-4af8-8002-d9a688b4aa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(l1 == expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe4f76-852a-4596-b794-10197d2c7f19",
   "metadata": {},
   "source": [
    "# ‚ùì Difference Between `NLTKWordTokenizer().tokenize()` and `TreebankWordTokenizer().tokenize()` in NLTK\n",
    "\n",
    "## üß† Short Answer:\n",
    "There is **no functional difference** between `NLTKWordTokenizer` and `TreebankWordTokenizer`.  \n",
    "üëâ `NLTKWordTokenizer` is simply a **wrapper** or **alias** for `TreebankWordTokenizer` ‚Äî designed to give a more consistent naming style across the NLTK package.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Background\n",
    "\n",
    "NLTK provides various tokenizers. Among them:\n",
    "\n",
    "- `TreebankWordTokenizer`  \n",
    "  ‚û§ A tokenizer that uses the Penn Treebank tokenization rules (splits contractions, punctuation, etc.).\n",
    "\n",
    "- `NLTKWordTokenizer`  \n",
    "  ‚û§ A more **user-friendly alias** of the same tokenizer, aligning with NLTK's naming conventions (like `NLTKSentenceTokenizer`, `NLTKTokenizer`).\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Let‚Äôs See with Code\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import TreebankWordTokenizer, NLTKWordTokenizer\n",
    "\n",
    "text = \"Mr. O'Neill can't attend the 3:00 p.m. meeting.\"\n",
    "\n",
    "# Both tokenizers\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "nltk_tokenizer = NLTKWordTokenizer()\n",
    "\n",
    "# Tokenizing\n",
    "tokens_treebank = treebank_tokenizer.tokenize(text)\n",
    "tokens_nltk = nltk_tokenizer.tokenize(text)\n",
    "\n",
    "# Check equality\n",
    "print(\"Are both outputs equal?\", tokens_treebank == tokens_nltk)\n",
    "\n",
    "# Print tokens\n",
    "print(\"\\nTreebank Tokens:\\n\", tokens_treebank)\n",
    "print(\"\\nNLTKWordTokenizer Tokens:\\n\", tokens_nltk)\n",
    "```\n",
    "---\n",
    "\n",
    "## Are both outputs equal? -> True \n",
    "- Treebank Tokens:\n",
    "<code>['Mr.', 'O', \"''\", 'Neill', 'ca', \"n't\", 'attend', 'the', '3:00', 'p.m.', 'meeting', '.']</code>\n",
    "\n",
    "- NLTKWordTokenizer Tokens:\n",
    "<code>['Mr.', 'O', \"''\", 'Neill', 'ca', \"n't\", 'attend', 'the', '3:00', 'p.m.', 'meeting', '.']</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae9e9f-d87c-44c8-af17-5eacf53057ff",
   "metadata": {},
   "source": [
    "## üõ† Behind the Scenes\n",
    "### üîç NLTKWordTokenizer is defined as:\n",
    "```python\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "class NLTKWordTokenizer(TreebankWordTokenizer):\n",
    "    \"\"\"This is just a renamed alias class for consistency.\"\"\"\n",
    "    pass\n",
    "\n",
    "```\n",
    "\n",
    "### So, NLTKWordTokenizer inherits directly from TreebankWordTokenizer, without changing any logic.\n",
    "\n",
    "## ‚úÖ Final Conclusion\n",
    "Use either class ‚Äî they behave identically.\n",
    "\n",
    "If you prefer more technical or research-based naming, use:\n",
    "TreebankWordTokenizer()\n",
    "\n",
    "If you want to keep naming consistent with other NLTK tokenizers, use:\n",
    "NLTKWordTokenizer()\n",
    "\n",
    "# ‚û°Ô∏è Both are 100% interchangeable in functionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
