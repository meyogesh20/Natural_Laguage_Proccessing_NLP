{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "576a47c0-f42c-418c-82c2-5fc6c695d433",
   "metadata": {},
   "source": [
    "# Stemming with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a98a1fc-4199-44bb-bf5b-81647dd79876",
   "metadata": {},
   "source": [
    "```\n",
    "# ! pip install nltk\n",
    "```\n",
    "- Stemming is somewhat crude method for cataloging related words; it essentially trim off letters from the end until the stem is reached.\n",
    "- This works fairly well in most cases, but unfortunately English has many exceptions where a more sophisticated process is required.\n",
    "- In fact, spaCy doesn't include a stemmer, opting instead to rely entirely on lemmatization\n",
    "- Because of this decision to not include Stemming in Spacy, we will jump over to using NLTK and learn about various Stemmers.\n",
    "- let's discuss both the Porter Stemmer and the Snowball Stemmer.\n",
    "  \n",
    "# Stemming with Porter\n",
    "- One of the most common and effective stemming tools, is Porter's Algorithm developed by Martin Porter in 1980\n",
    "- The algorithm employs five phases of word reduction, each with its own set of mapping rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d279028-9cbb-4528-9682-61743a440bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2724da6-4a59-4e84-a993-20babbdb809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6eb5cd1-e975-47aa-b354-0b194d3b138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run','ran','runner','running','runs','easily','fairly','mainly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2f56381-1582-4769-82a9-184ffcbfe4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous ---> gener\n",
      "generation ---> gener\n",
      "generously ---> gener\n",
      "generate ---> gener\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word, \"--->\",p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc6607-a5c5-4963-a625-a6d487e17063",
   "metadata": {},
   "source": [
    "# Stemming with Snowball\n",
    "- Snowball is the name fo a stemming language also developed by Martin Porter.\n",
    "- the algorithm used here is more accurately called the \"English Stemmer\" or \"Porter2 Stemmer\".\n",
    "- It offfers a slight improvement over the original Porter stemmer, both in logic and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68372d1d-6e23-4374-9b75-1bfe46f42df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee3bbc7e-63fe-4b74-8beb-3d470355ffc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ---> run\n",
      "ran ---> ran\n",
      "runner ---> runner\n",
      "running ---> run\n",
      "runs ---> run\n",
      "easily ---> easili\n",
      "fairly ---> fair\n",
      "mainly ---> main\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word, \"--->\", s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "566d4832-8186-477e-8424-0056e1a96247",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['generous','generation','generously','generate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32a136b1-ca7d-47f6-ab8e-8b3669ac416b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous ---> generous\n",
      "generation ---> generat\n",
      "generously ---> generous\n",
      "generate ---> generat\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word, \"--->\", s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623da61f-fd6d-4818-ad75-964f3fa08741",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "1. In contrast to stemming, **lemmatization** looks beyond word reduction, and considers a langauage's full vocabulory to apply a morphological analysis to words.\n",
    "2. For Exmpale<br>\n",
    "   - was --> be<br>\n",
    "   - mice --> mouse<br>\n",
    "   - meeting --> 'meet' or 'meeting' depending on its use in a sentence.\n",
    "4. _**Lemmatization**_ is typically seen as much more informative than simple stemming, which is why Spacy has opted to only have *Lemmatization* available instead of *Stemming*\n",
    "5. Lemmatization looks at surrounding text to determine a given word's part of speech, it does not categorize phrases.\n",
    "6. Next we will see word vectors and similarity.\n",
    "\n",
    "## Lemmatization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "092061fc-d751-4af0-93ee-aa00eeffebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c74b3f7-8a97-44ec-84c2-910729465817",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"I am a runner running in a race because I love to run since I ran today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "169bfde1-825f-4122-98bf-f3758959d40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t\t PRON \t 4690420944186131903 \t I\n",
      "am \t\t AUX \t 10382539506755952630 \t be\n",
      "a \t\t DET \t 11901859001352538922 \t a\n",
      "runner \t\t NOUN \t 12640964157389618806 \t runner\n",
      "running \t\t VERB \t 12767647472892411841 \t run\n",
      "in \t\t ADP \t 3002984154512732771 \t in\n",
      "a \t\t DET \t 11901859001352538922 \t a\n",
      "race \t\t NOUN \t 8048469955494714898 \t race\n",
      "because \t\t SCONJ \t 16950148841647037698 \t because\n",
      "I \t\t PRON \t 4690420944186131903 \t I\n",
      "love \t\t VERB \t 3702023516439754181 \t love\n",
      "to \t\t PART \t 3791531372978436496 \t to\n",
      "run \t\t VERB \t 12767647472892411841 \t run\n",
      "since \t\t SCONJ \t 10066841407251338481 \t since\n",
      "I \t\t PRON \t 4690420944186131903 \t I\n",
      "ran \t\t VERB \t 12767647472892411841 \t run\n",
      "today \t\t NOUN \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token.text,'\\t\\t',token.pos_,'\\t',token.lemma,'\\t',token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "60c062db-d7b8-40ec-b20f-bc213b77e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{15}} {token.pos_:{10}} {token.lemma:<{25}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d5cf242d-e571-448e-9fda-4f36c8c433c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I               PRON       4690420944186131903       I\n",
      "am              AUX        10382539506755952630      be\n",
      "a               DET        11901859001352538922      a\n",
      "runner          NOUN       12640964157389618806      runner\n",
      "running         VERB       12767647472892411841      run\n",
      "in              ADP        3002984154512732771       in\n",
      "a               DET        11901859001352538922      a\n",
      "race            NOUN       8048469955494714898       race\n",
      "because         SCONJ      16950148841647037698      because\n",
      "I               PRON       4690420944186131903       I\n",
      "love            VERB       3702023516439754181       love\n",
      "to              PART       3791531372978436496       to\n",
      "run             VERB       12767647472892411841      run\n",
      "since           SCONJ      10066841407251338481      since\n",
      "I               PRON       4690420944186131903       I\n",
      "ran             VERB       12767647472892411841      run\n",
      "today           NOUN       11042482332948150395      today\n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc1)\n",
    "# token.lemma --> returns hashvalue for that lemma in spacy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b07e762b-c955-4956-8e93-1634ebf5467d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dco2 \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI saw a ten mice today\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m show_lemmas(\u001b[43mdoc2\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc2' is not defined"
     ]
    }
   ],
   "source": [
    "dco2 = nlp(\"I saw a ten mice today\")\n",
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8192c3-31bd-4fab-8663-45f3479ef2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d9324f-2120-4365-be69-5006a4ef48a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-env]",
   "language": "python",
   "name": "conda-env-nlp-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
