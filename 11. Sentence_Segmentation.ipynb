{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a313a50-47fa-4b41-a908-66a14e01742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Standard imports\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06022c90-d362-438a-8292-c744dd7bb601",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is the first sentence. This is the another sentence. This one is the last sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383e4de9-5565-4f0b-afac-f1af8024885a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is the another sentence.\n",
      "This one is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5286297b-3e9a-4db5-9502-c704f3e21066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6623867-8035-4f2e-81dd-ca5cbb274ea8",
   "metadata": {},
   "source": [
    "```python\n",
    "doc.sents[0]\n",
    "# TypeError: 'generator' object is not subscriptable\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b1673f7-b133-4a3d-a611-86029ca7646b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This is the first sentence."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " list(doc.sents)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ca06ff2-26bd-4086-ba64-6110e0c7061f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(doc.sents)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e2ed1f7-67f4-47f0-9b50-269669521ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp('Management is doing the right things; leadership is doing the right thigns.\"  -Peter Drucker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c461d66a-65d9-4d67-a68b-4a11431b4e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Management is doing the right things; leadership is doing the right thigns.\"  -Peter Drucker'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a5c65fc-443a-421e-93e3-1c9e55eb213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Management is doing the right things; leadership is doing the right thigns.\"  \n",
      "-Peter Drucker\n"
     ]
    }
   ],
   "source": [
    "for sent in doc1.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe440463-6a59-40f7-ad2e-c145d19bd4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Management is doing the right things; leadership is doing the right thigns.\"  -Peter"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upto but not including last token\n",
    "doc1[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f56b98-4bab-4bb7-b796-a4fa702a84bc",
   "metadata": {},
   "source": [
    "## Add a sengmentation Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95f910e9-4c44-446a-b02d-a7337d2761fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëâ This is a test.\n",
      "üëâ e.g.\n",
      "üëâ we test custom splitting.\n",
      "üëâ Note: this should also split.\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ‚úÖ Register the function as a spaCy component\n",
    "@Language.component(\"custom_segmentation\")\n",
    "def custom_segmentation(doc):\n",
    "    for i, token in enumerate(doc[:-1]):\n",
    "        if token.text in (\"e.g.\", \"Note:\"):\n",
    "            doc[i + 1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "# ‚úÖ Add the custom component before the parser\n",
    "nlp.add_pipe(\"custom_segmentation\", before=\"parser\")\n",
    "\n",
    "# Test text\n",
    "text = \"This is a test. e.g. we test custom splitting. Note: this should also split.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the segmented sentences\n",
    "for sent in doc.sents:\n",
    "    print(\"üëâ\", sent.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26833621-e1e9-43c1-968f-3bd0ec0a0f03",
   "metadata": {},
   "source": [
    "## üîç Why does spaCy tokenize \"Note:\" into two tokens?\n",
    "- Even though \"e.g.\" is a single token, \"Note:\" is two tokens:\n",
    "    1. \"Note\"\n",
    "    2.  \":\"\n",
    "- This is by design, due to how spaCy‚Äôs tokenizer handles punctuation and known abbreviations.\n",
    "---\n",
    "\n",
    "# ‚úÖ spaCy Tokenizer Logic\n",
    "- spaCy‚Äôs tokenizer uses rules based on:\n",
    "    1. prefixes (e.g., $, (, \")\n",
    "    2. suffixes (e.g., ., !, ?, :)\n",
    "    3. infixes (e.g., -, /, ')\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53f2f02d-a651-4366-8eb9-89467cd70bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Note'\n",
      "':'\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Note:\")\n",
    "for token in doc:\n",
    "    print(repr(token.text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bec7f60-aa8d-4386-8bd5-bbaf4cc54901",
   "metadata": {},
   "source": [
    "# üß† Why?\n",
    "- : is a suffix punctuation, so it gets split.\n",
    "\n",
    "- spaCy doesn't treat \"Note:\" as an abbreviation like \"e.g.\", \"i.e.\", or \"Dr.\".\n",
    "\n",
    "# ‚úÖ Why is \"e.g.\" a Single Token Then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a276b87-0f57-43a9-b717-468c6bbfb073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'e.g.'\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"e.g.\")\n",
    "for token in doc:\n",
    "    print(repr(token.text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06bd6be-4188-4f6b-9e92-97e227c46773",
   "metadata": {},
   "source": [
    "- Because: \n",
    "    1. \"e.g.\" is in spaCy‚Äôs abbreviation exceptions\n",
    "    2. spaCy doesn't split on periods inside known abbreviations\n",
    "---\n",
    "# üõ†Ô∏è How to Treat \"Note:\" as a Single Token?\n",
    "- You can override the tokenizer if you really want \"Note:\" as one token: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1112777b-4d7e-46ac-865d-fb51c41f4222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note:\n",
      "this\n",
      "is\n",
      "a\n",
      "test\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Add custom tokenizer rule to treat \"Note:\" as a single token\n",
    "nlp.tokenizer.add_special_case(\"Note:\", [{ORTH: \"Note:\"}])\n",
    "\n",
    "doc = nlp(\"Note: this is a test.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74178ed-d420-439a-860d-43fae4d51de9",
   "metadata": {},
   "source": [
    "### Now \"Note:\" is treated like a single token ‚úÖ\n",
    "---\n",
    "# ‚úÖ Summary\n",
    "| Phrase    | Default Tokens  | Why?                          |\n",
    "| --------- | --------------- | ----------------------------- |\n",
    "| `\"e.g.\"`  | `'e.g.'`        | Known abbreviation, not split |\n",
    "| `\"Note:\"` | `'Note'`, `':'` | Colon is suffix punctuation   |\n",
    "| `\"Dr.\"`   | `'Dr.'`         | Known abbreviation            |\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ When to Use Special Cases?\n",
    "- Use add_special_case() if:\n",
    "    1. You want to treat \"Note:\", \"PS:\", \"FYI:\", etc., as atomic units\n",
    "    2. You're customizing sentence segmentation or entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba707e-df52-4280-98d9-9ec6de7b47b6",
   "metadata": {},
   "source": [
    "# üîç Explanation for Custom Segementation Component\n",
    "| Step                          | Purpose                                                       |\n",
    "| ----------------------------- | ------------------------------------------------------------- |\n",
    "| `@Language.component(\"name\")` | Registers your function in spaCy's component registry         |\n",
    "| `\"custom_segmentation\"`       | Name of your component when adding with `nlp.add_pipe(...)`   |\n",
    "| `before=\"parser\"`             | Ensures sentence boundaries are set before dependency parsing |\n",
    "\n",
    "# ‚ö†Ô∏è Note on Pipeline Order\n",
    " - Custom sentence segmenters must be placed before components like _**\"parser\"**_ or _**\"ner\"**_ because:\n",
    " - Sentence boundaries affect how parsing and NER behave\n",
    " - Use <code>nlp.add_pipe(custom_component, before=\"parser\")</code> for best results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2f19ac2-f791-4a4c-9576-143859d23e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëâ This is a test.\n",
      "üëâ e.g.\n",
      "üëâ we test custom splitting.\n",
      "üëâ Note:\n",
      "üëâ this should also split.\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "# from spacy.language import Language\n",
    "\n",
    "# # Load model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Register custom component\n",
    "@Language.component(\"custom_segmentation1\")\n",
    "def custom_segmentation(doc):\n",
    "    # Iterate and detect pattern: Note :\n",
    "    for i in range(len(doc) - 2):\n",
    "        if doc[i].text == \"Note\" and doc[i + 1].text == \":\":\n",
    "            # Set \"Note\" as sentence start (optional)\n",
    "            doc[i].is_sent_start = True\n",
    "            # ‚úÖ Force next token after \":\" as sentence start\n",
    "            doc[i + 2].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "# Add before parser\n",
    "nlp.add_pipe(\"custom_segmentation1\", before=\"parser\")\n",
    "\n",
    "# Text\n",
    "text = \"This is a test. e.g. we test custom splitting. Note: this should also split.\"\n",
    "\n",
    "# Apply\n",
    "doc = nlp(text)\n",
    "\n",
    "# Output\n",
    "for sent in doc.sents:\n",
    "    print(\"üëâ\", sent.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79248668-575c-44d0-910c-79d3ed991117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'custom_segmentation',\n",
       " 'custom_segmentation1',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ebc19d02-d003-4fb4-83eb-d3f9be7349ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first. \n",
      "\n",
      " \n",
      "This is the second\n",
      "\n",
      ".\n",
      "This is the third.\n",
      " \n",
      "This is the fourth.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('This is the first. \\n\\n This is the second\\n\\n. This is the third.\\n This is the fourth.')\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c22c6e-1e79-493c-9936-e0c0352f7e3d",
   "metadata": {},
   "source": [
    "# Use Custom Sentence Boundary Detection \n",
    "- via retokenizer and custom_sentencizer\n",
    "- You can write your own function to decide sentence boundaries using custom rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2588ab32-e610-45da-9643-1223c55c9295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "Note: this should start a new sentence.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"custom_sentencizer\")\n",
    "def custom_sentencizer(doc):\n",
    "    for i, token in enumerate(doc[:-1]):\n",
    "        if token.text == \"Note:\":\n",
    "            doc[i+1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")  # Ensure basic sentence splitting\n",
    "nlp.add_pipe(\"custom_sentencizer\", after=\"sentencizer\")\n",
    "\n",
    "doc = nlp(\"This is a sentence. Note: this should start a new sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce0459-7d78-4b58-a93b-fee1510cc4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c083b1da-3978-4fe3-b494-b8d276e69ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a25cfa-e78f-44b9-8839-a319c8478993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca8dc6-d87e-4854-b0c0-fc20f8863102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-env]",
   "language": "python",
   "name": "conda-env-nlp-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
